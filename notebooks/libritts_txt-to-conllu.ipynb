{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arh234/anaconda3/envs/stanza/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "nb_start = datetime.now()\n",
    "log_path = Path('data/libritts.log')\n",
    "dep_path = Path('data/libritts.conllu')\n",
    "input_path = Path(\"data/normalized_sample.txt\")\n",
    "#test input_path\n",
    "input_path.is_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the processors you want, which can speed things up. \n",
    "The processors required for dependency parsing are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 15:17:07 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-03-29 15:17:07 INFO: Use device: cpu\n",
      "2022-03-29 15:17:07 INFO: Loading: tokenize\n",
      "2022-03-29 15:17:07 INFO: Loading: pos\n",
      "2022-03-29 15:17:07 INFO: Loading: lemma\n",
      "2022-03-29 15:17:07 INFO: Loading: depparse\n",
      "2022-03-29 15:17:08 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "restricted_nlp = stanza.Pipeline(\n",
    "    lang='en',\n",
    "    processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">`standfordNLP` does not have multi-word token (`mwt`) expansion\n",
    ">for English, so `mwt` processor is not required for \n",
    ">dependency parsing, as discussed \n",
    ">[here](https://github.com/stanfordnlp/stanza/issues/297#issuecomment-627673245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = stanza.Pipeline('en')\n",
    "nlp = restricted_nlp\n",
    "filenames = []\n",
    "sents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Process file input: \n",
    " The test file I used here is not the architecture you were using, but just the output of `head *normalized*` in one of the train data directories, `103/1241/`. So it's a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with input_path.open(mode='r', encoding='utf8') as ifile:\n",
    "    for line in ifile:\n",
    "        if line.startswith('==> '): \n",
    "            filename = line.strip('\\n').strip('==>').strip(' ')\n",
    "            filenames.append(filename)\n",
    "            continue\n",
    "\n",
    "        # with open(filename, \"r\", encoding='utf-8') as sfile:\n",
    "        else:\n",
    "            # don't need to remove whitespace characters or line breaks for stanza\n",
    "            # unless they occurr midsentence, which might mess up the parsing \n",
    "            # (i.e. if stanza thinks they are different sentences)\n",
    "            sents.append(line#.strip('\\n')\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print it pretty just to see what the data is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103_1241_000000_000001.normalized.txt:\n",
      "  + matthew Cuthbert is surprised\n",
      "\n",
      "103_1241_000004_000002.normalized.txt:\n",
      "  + In fact, he had looked at twenty very much as he looked at sixty, lacking a little of the grayness.\n",
      "\n",
      "103_1241_000007_000001.normalized.txt:\n",
      "  + \"But there was a passenger dropped off for you-a little girl.\n",
      "\n",
      "103_1241_000008_000001.normalized.txt:\n",
      "  + \"It's a boy I've come for.\n",
      "\n",
      "103_1241_000012_000002.normalized.txt:\n",
      "  + Maybe they were out of boys of the brand you wanted.\"\n",
      "\n",
      "103_1241_000014_000003.normalized.txt:\n",
      "  + Her face was small, white and thin, also much freckled; her mouth was large and so were her eyes, which looked green in some lights and moods and gray in others.\n",
      "\n",
      "103_1241_000017_000000.normalized.txt:\n",
      "  + \"I suppose you are mr matthew Cuthbert of Green Gables?\" she said in a peculiarly clear, sweet voice.\n",
      "\n",
      "103_1241_000017_000001.normalized.txt:\n",
      "  + \"I'm very glad to see you.\n",
      "\n",
      "103_1241_000020_000000.normalized.txt:\n",
      "  + \"Oh, I can carry it,\" the child responded cheerfully.\n",
      "\n",
      "103_1241_000020_000001.normalized.txt:\n",
      "  + \"It isn't heavy. I've got all my worldly goods in it, but it isn't heavy.\n",
      "\n",
      "103_1241_000020_000005.normalized.txt:\n",
      "  + We've got to drive a long piece, haven't we?\n",
      "\n",
      "103_1241_000020_000016.normalized.txt:\n",
      "  + They were good, you know-the asylum people.\n",
      "\n",
      "103_1241_000021_000000.normalized.txt:\n",
      "  + With this Matthew's companion stopped talking, partly because she was out of breath and partly because they had reached the buggy.\n",
      "\n",
      "103_1241_000025_000007.normalized.txt:\n",
      "  + And I've never had a pretty dress in my life that I can remember-but of course it's all the more to look forward to, isn't it?\n",
      "\n",
      "103_1241_000025_000008.normalized.txt:\n",
      "  + And then I can imagine that I'm dressed gorgeously.\n",
      "\n",
      "103_1241_000025_000013.normalized.txt:\n",
      "  + When we got on the train I felt as if everybody must be looking at me and pitying me.\n",
      "\n",
      "103_1241_000025_000016.normalized.txt:\n",
      "  + I wasn't a bit sick coming over in the boat.\n",
      "\n",
      "103_1241_000025_000019.normalized.txt:\n",
      "  + She said she never saw the beat of me for prowling about.\n",
      "\n",
      "103_1241_000025_000028.normalized.txt:\n",
      "  + She said I must have asked her a thousand already.\n",
      "\n",
      "103_1241_000025_000030.normalized.txt:\n",
      "  + And what DOES make the roads red?\"\n",
      "\n",
      "103_1241_000026_000000.normalized.txt:\n",
      "  + \"Well now, I dunno,\" said matthew.\n",
      "\n",
      "103_1241_000027_000000.normalized.txt:\n",
      "  + \"Well, that is one of the things to find out sometime.\n",
      "\n",
      "103_1241_000027_000002.normalized.txt:\n",
      "  + It just makes me feel glad to be alive-it's such an interesting world.\n",
      "\n",
      "103_1241_000027_000005.normalized.txt:\n",
      "  + But am I talking too much?\n",
      "\n",
      "103_1241_000027_000007.normalized.txt:\n",
      "  + Would you rather I didn't talk?\n",
      "\n",
      "103_1241_000027_000009.normalized.txt:\n",
      "  + I can STOP when I make up my mind to it, although it's difficult.\"\n",
      "\n",
      "103_1241_000028_000003.normalized.txt:\n",
      "  + Women were bad enough in all conscience, but little girls were worse.\n",
      "\n",
      "103_1241_000029_000001.normalized.txt:\n",
      "  + I don't mind.\"\n",
      "\n",
      "103_1241_000030_000000.normalized.txt:\n",
      "  + \"Oh, I'm so glad.\n",
      "\n",
      "103_1241_000032_000001.normalized.txt:\n",
      "  + But it isn't-it's firmly fastened at one end.\n",
      "\n",
      "103_1241_000032_000003.normalized.txt:\n",
      "  + I asked her all about it.\n",
      "\n",
      "103_1241_000032_000011.normalized.txt:\n",
      "  + But you can't where you are.\n",
      "\n",
      "103_1241_000034_000000.normalized.txt:\n",
      "  + \"Fancy.\n",
      "\n",
      "103_1241_000034_000002.normalized.txt:\n",
      "  + I never expected I would, though.\n",
      "\n",
      "103_1241_000034_000005.normalized.txt:\n",
      "  + I can't feel exactly perfectly happy because-well, what color would you call this?\"\n",
      "\n",
      "103_1241_000038_000001.normalized.txt:\n",
      "  + \"Now you see why I can't be perfectly happy.\n",
      "\n",
      "103_1241_000038_000013.normalized.txt:\n",
      "  + I never could find out.\n",
      "\n",
      "103_1241_000038_000014.normalized.txt:\n",
      "  + Can you tell me?\"\n",
      "\n",
      "103_1241_000039_000000.normalized.txt:\n",
      "  + \"Well now, I'm afraid I can't,\" said matthew, who was getting a little dizzy.\n",
      "\n",
      "103_1241_000040_000000.normalized.txt:\n",
      "  + \"Well, whatever it was it must have been something nice because she was divinely beautiful.\n",
      "\n",
      "103_1241_000040_000001.normalized.txt:\n",
      "  + Have you ever imagined what it must feel like to be divinely beautiful?\"\n",
      "\n",
      "103_1241_000042_000001.normalized.txt:\n",
      "  + Which would you rather be if you had the choice-divinely beautiful or dazzlingly clever or angelically good?\"\n",
      "\n",
      "103_1241_000044_000002.normalized.txt:\n",
      "  + It's certain I'll never be angelically good.\n",
      "\n",
      "103_1241_000044_000003.normalized.txt:\n",
      "  + mrs Spencer says-oh, mr Cuthbert!\n",
      "\n",
      "103_1241_000044_000004.normalized.txt:\n",
      "  + Oh, mr Cuthbert!!\n",
      "\n",
      "103_1241_000044_000005.normalized.txt:\n",
      "  + Oh, mr Cuthbert!!!\"\n",
      "\n",
      "103_1241_000045_000001.normalized.txt:\n",
      "  + They had simply rounded a curve in the road and found themselves in the \"Avenue.\"\n",
      "\n",
      "103_1241_000052_000006.normalized.txt:\n",
      "  + Did you ever have an ache like that, mr Cuthbert?\"\n",
      "\n",
      "103_1241_000053_000000.normalized.txt:\n",
      "  + \"Well now, I just can't recollect that I ever had.\"\n",
      "\n",
      "103_1241_000054_000007.normalized.txt:\n",
      "  + Other people may call that place the Avenue, but I shall always call it the White Way of Delight. Have we really only another mile to go before we get home?\n",
      "\n",
      "103_1241_000054_000010.normalized.txt:\n",
      "  + Something still pleasanter may come after, but you can never be sure.\n",
      "\n",
      "103_1241_000054_000013.normalized.txt:\n",
      "  + But I'm glad to think of getting home.\n",
      "\n",
      "103_1241_000056_000000.normalized.txt:\n",
      "  + \"That's Barry's pond,\" said matthew.\n",
      "\n",
      "103_1241_000060_000001.normalized.txt:\n",
      "  + Do you think it can?\n",
      "\n",
      "103_1241_000060_000003.normalized.txt:\n",
      "  + But why do other people call it Barry's pond?\"\n",
      "\n",
      "103_1241_000061_000001.normalized.txt:\n",
      "  + Orchard Slope's the name of his place.\n",
      "\n",
      "103_1241_000066_000004.normalized.txt:\n",
      "  + So I shut my eyes.\n",
      "\n",
      "103_1241_000066_000006.normalized.txt:\n",
      "  + Because, you see, if the bridge DID crumple up I'd want to SEE it crumple.\n",
      "\n",
      "103_1241_000066_000008.normalized.txt:\n",
      "  + I always like the rumble part of it.\n",
      "\n",
      "103_1241_000066_000011.normalized.txt:\n",
      "  + Now I'll look back.\n",
      "\n",
      "103_1241_000069_000002.normalized.txt:\n",
      "  + I'm sure I'll guess right.\"\n",
      "\n",
      "103_1241_000071_000000.normalized.txt:\n",
      "  + \"That's it, isn't it?\" she said, pointing.\n",
      "\n",
      "103_1241_000073_000001.normalized.txt:\n",
      "  + But I reckon mrs Spencer described it so's you could tell.\"\n",
      "\n",
      "103_1241_000074_000000.normalized.txt:\n",
      "  + \"No, she didn't-really she didn't.\n",
      "\n",
      "103_1241_000074_000001.normalized.txt:\n",
      "  + All she said might just as well have been about most of those other places.\n",
      "\n",
      "103_1241_000074_000002.normalized.txt:\n",
      "  + I hadn't any real idea what it looked like.\n"
     ]
    }
   ],
   "source": [
    "for fn, s in zip(filenames,sents): \n",
    "    print(f'{fn}:\\n  + {s}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing step can be done from a list of sentences, as in `parse_list_of_sents()` below, but it can also be done from a whole block of text, as in `parse_multisentence_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_of_sents(sents, log_path, dep_path):\n",
    "    with log_path.open('a', encoding='utf8') as logstream:\n",
    "        \n",
    "        logstream.write(datetime.now().ctime())\n",
    "        logstream.write('\\n# Parsing from list of individual sentence strings'\n",
    "                        f'\\nsaving conllu formatted parsing to {dep_path}...')\n",
    "        \n",
    "        for sent in sents:\n",
    "            logstream.write('\\n'+sent)\n",
    "            # `sent` still has its original linebreak\n",
    "            #// log_path.write_text('\\n')\n",
    "            doc = nlp(sent)\n",
    "            \n",
    "            # These don't help in creating the grew-ready output, \n",
    "            #   only for processing within python\n",
    "            #// dicts = doc.to_dict()\n",
    "            #// conll = stanza.utils.conll.CoNLL.convert_dict(dicts)\n",
    "            #// logstream.write(str(len(conll))+'\\n')\n",
    "            logstream.write(f'\\n{len(doc.sentences)}\\n')\n",
    "\n",
    "            with dep_path.open('a', encoding='utf8') as depstream:\n",
    "                #// dep_path.write_text(str(conll))\n",
    "                depstream.write(stanza.utils.conll.CoNLL.doc2conll_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_multisentence_string(textstr, log_path, dep_path):\n",
    "    \n",
    "    with log_path.open('a', encoding='utf8') as logstream:\n",
    "\n",
    "        logstream.write(datetime.now().ctime())\n",
    "        logstream.write('\\n# Parsing from single string'\n",
    "                        f'\\nsaving conllu formatted parsing to {dep_path}...\\n')\n",
    "        logstream.write(textstr)\n",
    "        # `sent` still has its original linebreak\n",
    "        #// log_path.write_text('\\n')\n",
    "        doc = nlp(textstr)\n",
    "        \n",
    "        # These don't help in creating the grew-ready output, \n",
    "        #   only for processing within python\n",
    "        #// dicts = doc.to_dict()\n",
    "        #// conll = stanza.utils.conll.CoNLL.convert_dict(dicts)\n",
    "\n",
    "        logstream.write(f'\\n{len(doc.sentences)}\\n')\n",
    "\n",
    "        with dep_path.open('a', encoding='utf8') as depstream:\n",
    "            #// dep_path.write_text(str(conll))\n",
    "            depstream.write(stanza.utils.conll.CoNLL.doc2conll_text(doc))\n",
    "            depstream.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = datetime.now()\n",
    "\n",
    "parse_list_of_sents(\n",
    "    sents, log_path, \n",
    "    dep_path.with_name(f'from-list_{dep_path.name}'))\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "parse_multisentence_string(\n",
    "    ' '.join(sents), log_path, \n",
    "    dep_path.with_name(f'from-textblock_{dep_path.name}'))\n",
    "\n",
    "t2 = datetime.now()\n",
    "nb_code_end = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works the same: the output files are identical, and if you're really concerned about stanza messing up the sentence parsing that has already been done, a more obvious \"sentence break\" string delimiter can be used for `join`, e.g. `'\\n\\n'.join(sents)`. \n",
    "\n",
    "**Plus** it's at least 2x faster (if not more) to parse from the single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time from sentence list:\n",
      "  4.979 s\n",
      "\n",
      "time from textblock:\n",
      "  1.816 s\n",
      "\n",
      "processors in model:\n",
      "  <stanza.pipeline.tokenize_processor.TokenizeProcessor object at 0x7f4d6bfe8280>\n",
      "   <stanza.pipeline.pos_processor.POSProcessor object at 0x7f4d8c33a700>\n",
      "   <stanza.pipeline.lemma_processor.LemmaProcessor object at 0x7f4d4c1ca610>\n",
      "   <stanza.pipeline.depparse_processor.DepparseProcessor object at 0x7f4d4bcc81f0>\n",
      "total time: 8.085\n"
     ]
    }
   ],
   "source": [
    "list_time = t1 - t0\n",
    "block_time = t2 - t1\n",
    "list_time_message = f'\\ntime from sentence list:\\n  {round(list_time.total_seconds(), 3)} s'\n",
    "block_time_message = f'\\ntime from textblock:\\n  {round(block_time.total_seconds(), 3)} s'\n",
    "print(list_time_message + '\\n' + block_time_message)\n",
    "\n",
    "processors_string = '\\n   '.join(str(p) for p in nlp.loaded_processors)\n",
    "end_note = (f\"\\nprocessors in model:\\n  {processors_string}\"\n",
    "            f\"\\ntotal time: {round((nb_code_end-nb_start).total_seconds(), 3)} s\")\n",
    "print(end_note)\n",
    "\n",
    "with log_path.open('a') as logappend: \n",
    "    logappend.write(list_time_message)\n",
    "    logappend.write(block_time_message)\n",
    "    logappend.write(end_note)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b6c298d16f8950f09bc8c39b6c218d09af4e6be4b6effacfcdd31ba36dfd27b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('puddin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
